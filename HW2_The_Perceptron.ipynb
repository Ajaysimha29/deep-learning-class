{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **CSCE 5218 / CSCE 4930 Deep Learning**\n",
        "\n",
        "# **HW1a The Perceptron** (20 pt)\n"
      ],
      "metadata": {
        "id": "vYiZq0X2oB5t"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "source": [
        "# Get the datasets\n",
        "!wget http://huang.eng.unt.edu/CSCE-5218/test.dat\n",
        "!wget http://huang.eng.unt.edu/CSCE-5218/train.dat\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-02-16 20:53:52--  http://huang.eng.unt.edu/CSCE-5218/test.dat\n",
            "Resolving huang.eng.unt.edu (huang.eng.unt.edu)... 129.120.123.155\n",
            "Connecting to huang.eng.unt.edu (huang.eng.unt.edu)|129.120.123.155|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2844 (2.8K)\n",
            "Saving to: ‘test.dat’\n",
            "\n",
            "test.dat            100%[===================>]   2.78K  --.-KB/s    in 0s      \n",
            "\n",
            "2024-02-16 20:53:52 (247 MB/s) - ‘test.dat’ saved [2844/2844]\n",
            "\n",
            "--2024-02-16 20:53:52--  http://huang.eng.unt.edu/CSCE-5218/train.dat\n",
            "Resolving huang.eng.unt.edu (huang.eng.unt.edu)... 129.120.123.155\n",
            "Connecting to huang.eng.unt.edu (huang.eng.unt.edu)|129.120.123.155|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 11244 (11K)\n",
            "Saving to: ‘train.dat’\n",
            "\n",
            "train.dat           100%[===================>]  10.98K  --.-KB/s    in 0s      \n",
            "\n",
            "2024-02-16 20:53:53 (231 MB/s) - ‘train.dat’ saved [11244/11244]\n",
            "\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "vGVmKzgG2Ium",
        "outputId": "cdb76149-4598-4954-a89e-b312912dfcef"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "source": [
        "# Take a peek at the datasets\n",
        "!head train.dat\n",
        "!head test.dat"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A1\tA2\tA3\tA4\tA5\tA6\tA7\tA8\tA9\tA10\tA11\tA12\tA13\t\n",
            "1\t1\t0\t0\t0\t0\t0\t0\t1\t1\t0\t0\t1\t0\n",
            "0\t0\t1\t1\t0\t1\t1\t0\t0\t0\t0\t0\t1\t0\n",
            "0\t1\t0\t1\t1\t0\t1\t0\t1\t1\t1\t0\t1\t1\n",
            "0\t0\t1\t0\t0\t1\t0\t1\t0\t1\t1\t1\t1\t0\n",
            "0\t1\t0\t0\t0\t0\t0\t1\t1\t1\t1\t1\t1\t0\n",
            "0\t1\t1\t1\t0\t0\t0\t1\t0\t1\t1\t0\t1\t1\n",
            "0\t1\t1\t0\t0\t0\t1\t0\t0\t0\t0\t0\t1\t0\n",
            "0\t0\t0\t1\t1\t0\t1\t1\t1\t0\t0\t0\t1\t0\n",
            "0\t0\t0\t0\t0\t0\t1\t0\t1\t0\t1\t0\t1\t0\n",
            "A1\tA2\tA3\tA4\tA5\tA6\tA7\tA8\tA9\tA10\tA11\tA12\tA13\n",
            "1\t1\t1\t1\t0\t0\t1\t1\t0\t0\t0\t1\t1\t0\n",
            "0\t0\t0\t1\t0\t0\t1\t1\t0\t1\t0\t0\t1\t0\n",
            "0\t1\t1\t1\t0\t1\t1\t1\t1\t0\t0\t0\t1\t0\n",
            "0\t1\t1\t0\t1\t0\t1\t1\t1\t0\t1\t0\t1\t0\n",
            "0\t1\t0\t0\t0\t1\t0\t1\t0\t1\t0\t0\t1\t0\n",
            "0\t1\t1\t0\t0\t1\t1\t1\t1\t1\t1\t0\t1\t0\n",
            "0\t1\t1\t1\t0\t0\t1\t1\t0\t0\t0\t1\t1\t0\n",
            "0\t1\t0\t0\t1\t0\t0\t1\t1\t0\t1\t1\t1\t0\n",
            "1\t1\t1\t1\t0\t0\t1\t1\t0\t0\t0\t0\t1\t0\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "A69DxPSc8vNs",
        "outputId": "c7daf836-aa2c-478b-cbe6-94c1e56ac484"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Build the Perceptron Model\n",
        "\n",
        "You will need to complete some of the function definitions below.  DO NOT import any other libraries to complete this."
      ],
      "metadata": {
        "id": "rFXHLhnhwiBR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import itertools\n",
        "import re\n",
        "\n",
        "\n",
        "# Corpus reader, all columns but the last one are coordinates;\n",
        "#   the last column is the label\n",
        "def read_data(file_name):\n",
        "    f = open(file_name, 'r')\n",
        "\n",
        "    data = []\n",
        "    # Discard header line\n",
        "    f.readline()\n",
        "    for instance in f.readlines():\n",
        "        if not re.search('\\t', instance): continue\n",
        "        instance = list(map(int, instance.strip().split('\\t')))\n",
        "        # Add a dummy input so that w0 becomes the bias\n",
        "        instance = [-1] + instance\n",
        "        data += [instance]\n",
        "    return data\n",
        "\n",
        "\n",
        "def dot_product(array1, array2):\n",
        "    return sum(x * y for x, y in zip(array1, array2))\n",
        "\n",
        "\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + math.exp(-x))\n",
        "\n",
        "\n",
        "# The output of the model, which for the perceptron is\n",
        "# the sigmoid function applied to the dot product of\n",
        "# the instance and the weights\n",
        "def output(weight, instance):\n",
        "    return sigmoid(dot_product(weight, instance))\n",
        "\n",
        "\n",
        "# Predict the label of an instance; this is the definition of the perceptron\n",
        "# you should output 1 if the output is >= 0.5 else output 0\n",
        "def predict(weights, instance):\n",
        "    return 1 if output(weights, instance) >= 0.5 else 0\n",
        "\n",
        "\n",
        "# Accuracy = percent of correct predictions\n",
        "def get_accuracy(weights, instances):\n",
        "    correct = sum(1 if predict(weights, instance) == instance[-1] else 0\n",
        "                   for instance in instances)\n",
        "    return correct * 100 / len(instances)\n",
        "\n",
        "\n",
        "# Train a perceptron with instances and hyperparameters:\n",
        "#       lr (learning rate)\n",
        "#       epochs\n",
        "# The implementation comes from the definition of the perceptron\n",
        "#\n",
        "# Training consists on fitting the parameters which are the weights\n",
        "# that's the only thing training is responsible to fit\n",
        "# (recall that w0 is the bias, and w1..wn are the weights for each coordinate)\n",
        "#\n",
        "# Hyperparameters (lr and epochs) are given to the training algorithm\n",
        "# We are updating weights in the opposite direction of the gradient of the error,\n",
        "# so with a \"decent\" lr we are guaranteed to reduce the error after each iteration.\n",
        "def train_perceptron(instances, lr, epochs):\n",
        "  #weights initialize\n",
        "    weights = [0] * (len(instances[0])-1)\n",
        "\n",
        "    for _ in range(epochs):\n",
        "        for instance in instances:\n",
        "          #:calculate sum of weights\n",
        "            in_value = dot_product(weights, instance)\n",
        "            #output calucalation using sigmoid function\n",
        "            output_val = sigmoid(in_value)\n",
        "            #error calculation\n",
        "            error = instance[-1] - output_val\n",
        "            #updated weights\n",
        "            for i in range(0, len(weights)):\n",
        "                weights[i] += lr * error * output_val * (1 - output_val) * instance[i]\n",
        "\n",
        "    return weights\n",
        "\n",
        "# Testing the functions with some example data\n",
        "file_name = \"train.dat\"\n",
        "instances = read_data(file_name)\n",
        "lr = 0.1\n",
        "epochs = 20\n",
        "weights = train_perceptron(instances, lr, epochs)\n",
        "accuracy = get_accuracy(weights, instances)\n",
        "print(\"Accuracy:\", accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "yMcBzwdZsxah",
        "outputId": "b002d92c-cafc-4d66-fa7e-019c3c0cf4b7"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 83.75\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**dot product** :  dot product function is for calculating the weights and weighted sum of inputs.\n",
        "\n",
        "**sigmoid :**sigmoid function will generalize the patterns in the data or handles in the non-linearity and output should be in the range zero and one.\n"
      ],
      "metadata": {
        "id": "jRsMsTXI9lWU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run it"
      ],
      "metadata": {
        "id": "adBZuMlAwiBT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "source": [
        "instances_tr = read_data(\"train.dat\")\n",
        "instances_te = read_data(\"test.dat\")\n",
        "lr = 0.005\n",
        "epochs = 5\n",
        "weights = train_perceptron(instances_tr, lr, epochs)\n",
        "accuracy = get_accuracy(weights, instances_te)\n",
        "print(f\"#tr: {len(instances_tr):3}, epochs: {epochs:3}, learning rate: {lr:.3f}; \"\n",
        "      f\"Accuracy (test, {len(instances_te)} instances): {accuracy:.1f}\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#tr: 400, epochs:   5, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n"
          ]
        }
      ],
      "metadata": {
        "id": "50YvUza-BYQF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "5059aec9-6926-44bd-c395-147270d97000"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the code first we loaded the data and next we wrote the parameters like learning rate and no of epochs and we trained the model and next we calculated the accuracy and printed the following things mentioned in the code."
      ],
      "metadata": {
        "id": "U8i7H4I548S5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#diffrent parameters\n",
        "instances_tr = read_data(\"train.dat\")\n",
        "instances_te = read_data(\"test.dat\")\n",
        "lr = 0.050\n",
        "epochs = 100\n",
        "weights = train_perceptron(instances_tr, lr, epochs)\n",
        "accuracy = get_accuracy(weights, instances_te)\n",
        "print(f\"#tr: {len(instances_tr):3}, epochs: {epochs:3}, learning rate: {lr:.3f}; \"\n",
        "      f\"Accuracy (test, {len(instances_te)} instances): {accuracy:.1f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PFDUNWcC4svi",
        "outputId": "dd7a44f4-be4f-45d6-a570-93e61fac388f"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#tr: 400, epochs: 100, learning rate: 0.050; Accuracy (test, 100 instances): 80.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "accuracy got better because I changed the parameters values."
      ],
      "metadata": {
        "id": "Nc7QWelG49rr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Questions\n",
        "\n",
        "Answer the following questions. Include your implementation and the output for each question."
      ],
      "metadata": {
        "id": "CBXkvaiQMohX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "### Question 1\n",
        "\n",
        "In `train_perceptron(instances, lr, epochs)`, we have the follosing code:\n",
        "```\n",
        "in_value = dot_product(weights, instance)\n",
        "output = sigmoid(in_value)\n",
        "error = instance[-1] - output\n",
        "```\n",
        "\n",
        "Why don't we have the following code snippet instead?\n",
        "```\n",
        "output = predict(weights, instance)\n",
        "error = instance[-1] - output\n",
        "```\n",
        "\n",
        "#### TODO Add your answer here (text only)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "YCQ6BEk1CBlr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code is about finding the error between the actual label and predicted output of the perceptron model.\n",
        "The dot product line is about calculating the weighted sum of the inputs and corresponding weights. Sigmoid function is used to produce the output of the model. The code is sigmoid instead of predict directly because compute the error more accurately and by using the sigmoid we can ensure the output is between range of zero and one. Last line of code error will calculate the error by subtracting the predicted output from the actual label. Sigmoid function helps in computation of error more accurately for training the perceptron and model for better accuracy.\n"
      ],
      "metadata": {
        "id": "sxZu4KO_xRD_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Question 2\n",
        "Train the perceptron with the following hyperparameters and calculate the accuracy with the test dataset.\n",
        "\n",
        "```\n",
        "tr_percent = [5, 10, 25, 50, 75, 100] # percent of the training dataset to train with\n",
        "num_epochs = [5, 10, 20, 50, 100]              # number of epochs\n",
        "lr = [0.005, 0.01, 0.05]              # learning rate\n",
        "```\n",
        "\n",
        "TODO: Write your code below and include the output at the end of each training loop (NOT AFTER EACH EPOCH)\n",
        "of your code.The output should look like the following:\n",
        "```\n",
        "# tr:  20, epochs:   5, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
        "# tr:  20, epochs:  10, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
        "# tr:  20, epochs:  20, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
        "[and so on for all the combinations]\n",
        "```\n",
        "You will get different results with different hyperparameters.\n",
        "\n",
        "#### TODO Add your answer here (code and output in the format above)\n"
      ],
      "metadata": {
        "id": "JU3c3m6YL2rK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "instances_tr = read_data(\"train.dat\")\n",
        "instances_te = read_data(\"test.dat\")\n",
        "tr_percent = [5, 10, 25, 50, 75, 100] # percent of the training dataset to train with\n",
        "num_epochs = [5, 10, 20, 50, 100]     # number of epochs\n",
        "lr_array = [0.005, 0.01, 0.05]        # learning rate\n",
        "\n",
        "for lr in lr_array:\n",
        "    for tr_size in tr_percent:\n",
        "        for epochs in num_epochs:\n",
        "            size = round(len(instances_tr) * tr_size / 100)\n",
        "            pre_instances = instances_tr[:size]\n",
        "            weights = train_perceptron(pre_instances, lr, epochs)\n",
        "            accuracy = get_accuracy(weights, instances_te)\n",
        "            print(f\"# tr: {len(pre_instances)}, epochs: {epochs}, learning rate: {lr:.3f}; \"\n",
        "                  f\"Accuracy (test, {len(instances_te)} instances): {accuracy:.1f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "g1sfvOZ8yV60",
        "outputId": "2c91673a-0765-44bb-ad72-36e89bd72455"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# tr: 20, epochs: 5, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
            "# tr: 20, epochs: 10, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
            "# tr: 20, epochs: 20, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
            "# tr: 20, epochs: 50, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
            "# tr: 20, epochs: 100, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
            "# tr: 40, epochs: 5, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
            "# tr: 40, epochs: 10, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
            "# tr: 40, epochs: 20, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
            "# tr: 40, epochs: 50, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
            "# tr: 40, epochs: 100, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
            "# tr: 100, epochs: 5, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
            "# tr: 100, epochs: 10, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
            "# tr: 100, epochs: 20, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
            "# tr: 100, epochs: 50, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
            "# tr: 100, epochs: 100, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
            "# tr: 200, epochs: 5, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
            "# tr: 200, epochs: 10, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
            "# tr: 200, epochs: 20, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
            "# tr: 200, epochs: 50, learning rate: 0.005; Accuracy (test, 100 instances): 67.0\n",
            "# tr: 200, epochs: 100, learning rate: 0.005; Accuracy (test, 100 instances): 74.0\n",
            "# tr: 300, epochs: 5, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
            "# tr: 300, epochs: 10, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
            "# tr: 300, epochs: 20, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
            "# tr: 300, epochs: 50, learning rate: 0.005; Accuracy (test, 100 instances): 74.0\n",
            "# tr: 300, epochs: 100, learning rate: 0.005; Accuracy (test, 100 instances): 78.0\n",
            "# tr: 400, epochs: 5, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
            "# tr: 400, epochs: 10, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
            "# tr: 400, epochs: 20, learning rate: 0.005; Accuracy (test, 100 instances): 69.0\n",
            "# tr: 400, epochs: 50, learning rate: 0.005; Accuracy (test, 100 instances): 73.0\n",
            "# tr: 400, epochs: 100, learning rate: 0.005; Accuracy (test, 100 instances): 77.0\n",
            "# tr: 20, epochs: 5, learning rate: 0.010; Accuracy (test, 100 instances): 68.0\n",
            "# tr: 20, epochs: 10, learning rate: 0.010; Accuracy (test, 100 instances): 68.0\n",
            "# tr: 20, epochs: 20, learning rate: 0.010; Accuracy (test, 100 instances): 68.0\n",
            "# tr: 20, epochs: 50, learning rate: 0.010; Accuracy (test, 100 instances): 68.0\n",
            "# tr: 20, epochs: 100, learning rate: 0.010; Accuracy (test, 100 instances): 68.0\n",
            "# tr: 40, epochs: 5, learning rate: 0.010; Accuracy (test, 100 instances): 68.0\n",
            "# tr: 40, epochs: 10, learning rate: 0.010; Accuracy (test, 100 instances): 68.0\n",
            "# tr: 40, epochs: 20, learning rate: 0.010; Accuracy (test, 100 instances): 68.0\n",
            "# tr: 40, epochs: 50, learning rate: 0.010; Accuracy (test, 100 instances): 68.0\n",
            "# tr: 40, epochs: 100, learning rate: 0.010; Accuracy (test, 100 instances): 68.0\n",
            "# tr: 100, epochs: 5, learning rate: 0.010; Accuracy (test, 100 instances): 68.0\n",
            "# tr: 100, epochs: 10, learning rate: 0.010; Accuracy (test, 100 instances): 68.0\n",
            "# tr: 100, epochs: 20, learning rate: 0.010; Accuracy (test, 100 instances): 68.0\n",
            "# tr: 100, epochs: 50, learning rate: 0.010; Accuracy (test, 100 instances): 68.0\n",
            "# tr: 100, epochs: 100, learning rate: 0.010; Accuracy (test, 100 instances): 71.0\n",
            "# tr: 200, epochs: 5, learning rate: 0.010; Accuracy (test, 100 instances): 68.0\n",
            "# tr: 200, epochs: 10, learning rate: 0.010; Accuracy (test, 100 instances): 68.0\n",
            "# tr: 200, epochs: 20, learning rate: 0.010; Accuracy (test, 100 instances): 68.0\n",
            "# tr: 200, epochs: 50, learning rate: 0.010; Accuracy (test, 100 instances): 74.0\n",
            "# tr: 200, epochs: 100, learning rate: 0.010; Accuracy (test, 100 instances): 78.0\n",
            "# tr: 300, epochs: 5, learning rate: 0.010; Accuracy (test, 100 instances): 68.0\n",
            "# tr: 300, epochs: 10, learning rate: 0.010; Accuracy (test, 100 instances): 68.0\n",
            "# tr: 300, epochs: 20, learning rate: 0.010; Accuracy (test, 100 instances): 70.0\n",
            "# tr: 300, epochs: 50, learning rate: 0.010; Accuracy (test, 100 instances): 78.0\n",
            "# tr: 300, epochs: 100, learning rate: 0.010; Accuracy (test, 100 instances): 80.0\n",
            "# tr: 400, epochs: 5, learning rate: 0.010; Accuracy (test, 100 instances): 68.0\n",
            "# tr: 400, epochs: 10, learning rate: 0.010; Accuracy (test, 100 instances): 69.0\n",
            "# tr: 400, epochs: 20, learning rate: 0.010; Accuracy (test, 100 instances): 70.0\n",
            "# tr: 400, epochs: 50, learning rate: 0.010; Accuracy (test, 100 instances): 77.0\n",
            "# tr: 400, epochs: 100, learning rate: 0.010; Accuracy (test, 100 instances): 80.0\n",
            "# tr: 20, epochs: 5, learning rate: 0.050; Accuracy (test, 100 instances): 68.0\n",
            "# tr: 20, epochs: 10, learning rate: 0.050; Accuracy (test, 100 instances): 68.0\n",
            "# tr: 20, epochs: 20, learning rate: 0.050; Accuracy (test, 100 instances): 68.0\n",
            "# tr: 20, epochs: 50, learning rate: 0.050; Accuracy (test, 100 instances): 68.0\n",
            "# tr: 20, epochs: 100, learning rate: 0.050; Accuracy (test, 100 instances): 64.0\n",
            "# tr: 40, epochs: 5, learning rate: 0.050; Accuracy (test, 100 instances): 68.0\n",
            "# tr: 40, epochs: 10, learning rate: 0.050; Accuracy (test, 100 instances): 68.0\n",
            "# tr: 40, epochs: 20, learning rate: 0.050; Accuracy (test, 100 instances): 68.0\n",
            "# tr: 40, epochs: 50, learning rate: 0.050; Accuracy (test, 100 instances): 71.0\n",
            "# tr: 40, epochs: 100, learning rate: 0.050; Accuracy (test, 100 instances): 69.0\n",
            "# tr: 100, epochs: 5, learning rate: 0.050; Accuracy (test, 100 instances): 68.0\n",
            "# tr: 100, epochs: 10, learning rate: 0.050; Accuracy (test, 100 instances): 67.0\n",
            "# tr: 100, epochs: 20, learning rate: 0.050; Accuracy (test, 100 instances): 70.0\n",
            "# tr: 100, epochs: 50, learning rate: 0.050; Accuracy (test, 100 instances): 74.0\n",
            "# tr: 100, epochs: 100, learning rate: 0.050; Accuracy (test, 100 instances): 77.0\n",
            "# tr: 200, epochs: 5, learning rate: 0.050; Accuracy (test, 100 instances): 71.0\n",
            "# tr: 200, epochs: 10, learning rate: 0.050; Accuracy (test, 100 instances): 77.0\n",
            "# tr: 200, epochs: 20, learning rate: 0.050; Accuracy (test, 100 instances): 78.0\n",
            "# tr: 200, epochs: 50, learning rate: 0.050; Accuracy (test, 100 instances): 78.0\n",
            "# tr: 200, epochs: 100, learning rate: 0.050; Accuracy (test, 100 instances): 76.0\n",
            "# tr: 300, epochs: 5, learning rate: 0.050; Accuracy (test, 100 instances): 74.0\n",
            "# tr: 300, epochs: 10, learning rate: 0.050; Accuracy (test, 100 instances): 78.0\n",
            "# tr: 300, epochs: 20, learning rate: 0.050; Accuracy (test, 100 instances): 79.0\n",
            "# tr: 300, epochs: 50, learning rate: 0.050; Accuracy (test, 100 instances): 78.0\n",
            "# tr: 300, epochs: 100, learning rate: 0.050; Accuracy (test, 100 instances): 77.0\n",
            "# tr: 400, epochs: 5, learning rate: 0.050; Accuracy (test, 100 instances): 69.0\n",
            "# tr: 400, epochs: 10, learning rate: 0.050; Accuracy (test, 100 instances): 76.0\n",
            "# tr: 400, epochs: 20, learning rate: 0.050; Accuracy (test, 100 instances): 80.0\n",
            "# tr: 400, epochs: 50, learning rate: 0.050; Accuracy (test, 100 instances): 80.0\n",
            "# tr: 400, epochs: 100, learning rate: 0.050; Accuracy (test, 100 instances): 80.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "The code is about understanding the perceptron model which is mostly used for the classification and the code is about the performance of the model experimenting with the different values in hyperparameters to see the accuracies in the model. First in the code we used read syntax to load the data and then hyperparameters are mentioned in the code. Combinations of settings like how much the data with (tr_percent) and no of epochs and how quickly the model learns and by using different combinations of the parameters will help us in train the model to get better results and get good accuracy.\n"
      ],
      "metadata": {
        "id": "mUcAPIFc1OVW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Question 3\n",
        "Write a couple paragraphs interpreting the results with all the combinations of hyperparameters. Drawing a plot will probably help you make a point. In particular, answer the following:\n",
        "- A. Do you need to train with all the training dataset to get the highest accuracy with the test dataset?\n",
        "- B. How do you justify that training the second run obtains worse accuracy than the first one (despite the second one uses more training data)?\n",
        "   ```\n",
        "#tr: 100, epochs:  20, learning rate: 0.050; Accuracy (test, 100 instances): 71.0\n",
        "#tr: 200, epochs:  20, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
        "```\n",
        "- C. Can you get higher accuracy with additional hyperparameters (higher than `80.0`)?\n",
        "- D. Is it always worth training for more epochs (while keeping all other hyperparameters fixed)?\n",
        "\n",
        "#### TODO: Add your answer here (code and text)\n",
        "\n"
      ],
      "metadata": {
        "id": "OFB9MtwML24O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A. No, we don’t have to train the entire dataset entirely to get the best accuracy using the test data but, factors like data quality and model complexity and number of factors you are using to train the data will influence the link between quantity of the training dataset and test accuracy.\n",
        "\n",
        "B. The second run may lead to overfitting (the model will be great with train data but not great with test data because it will take the noise data while training the model). model became specialized to train data in the 2nd run and test dataset performance get reduced.\n",
        "\n",
        "C. Yes, we can improve the accuracy by adding more hyperparameters to the model. We can use regularization techniques, grid search and key hyperparameters play key role in improving model performance.\n",
        "\n",
        "D. No, we can’t always use more epochs because, normally it will generalize the patterns but we if use more epochs will lead to overfitting and model will learn from the noise data leads to the poor generalization and poor or less accuracy.\n",
        "\n"
      ],
      "metadata": {
        "id": "950f83QZshnq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "38rA_Kp3wiBX"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nqLSyA_n5Jy1"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}